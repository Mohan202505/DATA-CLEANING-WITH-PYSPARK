# Datacleaning#
ğŸš€ Data Cleaning with PySpark

This project showcases how to clean and preprocess a dataset using **Apache Spark (PySpark)** â€” a powerful big data engine. It includes handling of **missing values**, **removal of duplicate records**, and outputs summary statistics before and after cleaning.

---

## ğŸ“¦ Dataset Used

### `trip_data.csv`

A sample trip dataset containing:

| trip_id | origin        | destination   | distance_km | passenger_count |
|---------|---------------|---------------|-------------|-----------------|
| 1       | New York      | Boston        | 350         | 2               |
| 2       | San Francisco | Los Angeles   |             |                 |
| 3       | New York      | Boston        | 350         | 2               |
| 4       | Chicago       |               | 800         | 3               |
| 5       | Miami         | Orlando       | 380         |                 |
| 6       | Dallas        | Houston       | 390         | 4               |

---

## ğŸ§ª Features

- âœ… Load CSV dataset with header and schema inference
- ğŸ—‘ï¸ Remove duplicate entries using `dropDuplicates()`
- âŒ Identify missing/null values per column
- ğŸ’§ Drop rows containing any null values using `dropna()`
- ğŸ“‰ Display row counts before and after cleaning

---

## ğŸ§  Technologies Used

- Apache Spark
- PySpark (Python API for Spark)

---

## ğŸ› ï¸ Project Files

pyspark-data-cleaning/
â”œâ”€â”€ trip_data.csv # Sample input data
â”œâ”€â”€ data_cleaning_trip_pyspark.py # Python script with PySpark logic
â”œâ”€â”€ README.md # Project documentation

yaml
Copy code

---

## ğŸ“œ Script Overview: `data_cleaning_trip_pyspark.py`

```python
spark = SparkSession.builder.appName("Trip Data Cleaning").getOrCreate()
df = spark.read.csv("trip_data.csv", header=True, inferSchema=True)
df_cleaned = df.dropDuplicates().dropna()
df_cleaned.show()
spark.stop()
â–¶ï¸ How to Run the Script
ğŸ’» If using CLI with spark-submit:
bash
Copy code
spark-submit data_cleaning_trip_pyspark.py
ğŸ““ If using Jupyter/Colab:
Install PySpark:

bash
Copy code
pip install pyspark
Then run the code block by block.

ğŸ“Š Output Summary
Duplicates Removed: Trip ID 3 was a duplicate of ID 1

Missing Values Handled: Rows with missing distance or passenger count were dropped

Cleaned Rows: Only fully complete records retained

trip_id	origin	destination	distance_km	passenger_count
1	New York	Boston	350	2
6	Dallas	Houston	390	4

ğŸ§‘â€ğŸ’» Author
Mohan Reddy Krishna T

ğŸ“ License
This project is open-source and available for academic and learning purposes.
